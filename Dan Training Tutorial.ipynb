{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb173248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 21:41:38.935211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, Flatten, Dense, Softmax\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62384345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 21:41:39.529766: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-28 21:41:39.530152: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-11-28 21:41:39.532808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:39.532887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 4060 Ti computeCapability: 8.9\n",
      "coreClock: 2.655GHz coreCount: 34 deviceMemorySize: 15.71GiB deviceMemoryBandwidth: 268.25GiB/s\n",
      "2023-11-28 21:41:39.532907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:39.532956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:05:00.0 name: NVIDIA GeForce RTX 4060 Ti computeCapability: 8.9\n",
      "coreClock: 2.655GHz coreCount: 34 deviceMemorySize: 15.71GiB deviceMemoryBandwidth: 268.25GiB/s\n",
      "2023-11-28 21:41:39.532961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-28 21:41:39.534066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-28 21:41:39.534088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-28 21:41:39.534582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-11-28 21:41:39.534699: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-11-28 21:41:39.535852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-11-28 21:41:39.536146: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-11-28 21:41:39.536212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-28 21:41:39.536256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:39.536342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:39.536399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:39.536453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:39.536496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[1], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95ed0c",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "Open **dan_train.csv** file and split the games into a list.\n",
    "Every row of csv: `DL0000000001,B,B[pd],W[dp],B[pp],W[dc],B[de],...`. \n",
    "\n",
    "Columns are:\n",
    "\n",
    "    1. DL0000000001: Game ID\n",
    "    2. B: Player's color\n",
    "    3-... : Moves\n",
    "    \n",
    "We cropped only the moves to game list as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8872fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = open('./Training Dataset/dan_train.csv').read().splitlines()\n",
    "games = [i.split(',',2)[-1] for i in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58532b01",
   "metadata": {},
   "source": [
    "Create a dictionary to convert the coordinates from characters to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496585f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = 'abcdefghijklmnopqrs'\n",
    "coordinates = {k:v for v,k in enumerate(chars)}\n",
    "chartonumbers = {k:v for k,v in enumerate(chars)}\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92277370",
   "metadata": {},
   "source": [
    "We decided to build a DCNN model in this tutorial. We create data samples by using every move in every game, meaning that the target is to predict the next move by feeding the previous state of the table in every game for every move. Therefore, we can collect much more data samples from games.\n",
    "\n",
    "For the simplicity, we used 4 dimensional feature map to represent the data as below:\n",
    " 1. Positions of black stones: mark them as 1 and the rest of the table as 0\n",
    " 2. Positions of white stones: mark them as 1 and the rest of the table as 0\n",
    " 3. Empty areas of the table: mark the empty areas as 1 and occupied areas as 0\n",
    " 4. The last move in the table: mark the position of the last move as 1 and the rest as 0\n",
    " \n",
    "Target value is a number between 0-361(19\\*19). Later this will be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adb423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(moves):\n",
    "    x = np.zeros((19,19,11))\n",
    "    map = []\n",
    "    # x[:,:,6] = 1\n",
    "    for move in moves:\n",
    "        color = move[0]\n",
    "        column = coordinates[move[2]]\n",
    "        row = coordinates[move[3]]\n",
    "        map.append(row)\n",
    "        map.append(column)\n",
    "        if color == 'B':\n",
    "            x[row,column,0] = 1\n",
    "            x[row,column,2] = 1\n",
    "            # x[:,:,6] = 0\n",
    "        if color == 'W':\n",
    "            x[row,column,1] = 1\n",
    "            x[row,column,2] = 1\n",
    "            # x[:,:,6] = 0\n",
    "    if moves:\n",
    "        last_move_column = coordinates[moves[-1][2]]\n",
    "        last_move_row = coordinates[moves[-1][3]]\n",
    "        x[row,column,3] = 1\n",
    "\n",
    "        color = moves[-1][0]\n",
    "        if color == 'B':\n",
    "            x[last_move_row,last_move_column,4] = 1\n",
    "        else:\n",
    "            x[last_move_row,last_move_column,5] = 1\n",
    "\n",
    "        #最後一子的周邊分布\n",
    "        for dr, dc in [(last_move_row, last_move_column+1), (last_move_row, last_move_column-1), (last_move_row+1, last_move_column), (last_move_row-1, last_move_column)]:   \n",
    "            if 0 <= dr < 19 and 0<= dc < 19 and not x[dr,dc,0]:\n",
    "                x[dr,dc,10]=1\n",
    "        # #氣\n",
    "        # for dr, dc in [(last_move_row, last_move_column+1), (last_move_row, last_move_column-1), (last_move_row+1, last_move_column), (last_move_row-1, last_move_column)]:\n",
    "        #     if color == 'B':\n",
    "        #         if 0 <= dr < 19 and 0<= dc < 19 and not x[dr,dc,5]:\n",
    "        #             x[dr,dc,7]=1 \n",
    "        #     else:\n",
    "        #         if 0 <= dr < 19 and 0<= dc < 19 and not x[dr,dc,4]:\n",
    "        #             x[dr,dc,8]=1\n",
    "\n",
    "    for i in range(0, len(map), 2):\n",
    "        # if (map[i+1] == 0 and map[i]== 0) or (map[i+1] == 18 and map[i]== 18) or (map[i+1] == 0 and map[i]== 18) or (map[i+1] == 18 and map[i]== 0):\n",
    "        #     qi = 2\n",
    "        # elif map[i+1] == 0 or map[i+1] == 18 or map[i] == 0 or map[i] == 18:\n",
    "        #     qi = 3\n",
    "        # else:\n",
    "        qi = 4\n",
    "\n",
    "        for dr, dc in [(map[i], map[i+1]+1), (map[i], map[i+1]-1), (map[i]+1, map[i+1]), (map[i]-1, map[i+1])]:   \n",
    "            if 0 <= dr < 19 and 0<= dc < 19 and not x[dr,dc,0]:\n",
    "                qi-=1\n",
    "        if qi==1:\n",
    "            x[map[i],map[i+1],6]=1\n",
    "        if qi==2:\n",
    "            x[map[i],map[i+1],7]=1\n",
    "        if qi==3:\n",
    "            x[map[i],map[i+1],8]=1\n",
    "        if qi==4:\n",
    "            x[map[i],map[i+1],9]=1\n",
    "\n",
    "\n",
    "    x[:,:,2] = np.where(x[:,:,2] == 0, 1, 0)\n",
    "    return x\n",
    "\n",
    "def prepare_label(move):\n",
    "    column = coordinates[move[2]]\n",
    "    row = coordinates[move[3]]\n",
    "    return column*19+row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758808ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Games: 100160, Total Moves: 22853380\n"
     ]
    }
   ],
   "source": [
    "# Check how many samples can be obtained\n",
    "n_games = 0\n",
    "n_moves = 0\n",
    "for game in games:\n",
    "    n_games += 1\n",
    "    moves_list = game.split(',')\n",
    "    for move in moves_list:\n",
    "        n_moves += 1\n",
    "print(f\"Total Games: {n_games}, Total Moves: {n_moves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46403360",
   "metadata": {},
   "source": [
    "The code below is run for baseline model only by using only the first 500 games from the dataset. You might need to create a data generator to use complete dataset. Otherwise your RAM might not enough to store all (If you run the code on free version of Google Colab, it will crash above 500 game samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9bb0ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []\n",
    "# y = []\n",
    "# for game in games[:1]:\n",
    "#     moves_list = game.split(',')\n",
    "#     for count, move in enumerate(moves_list):\n",
    "#         x.append(prepare_input(moves_list[:count]))\n",
    "#         y.append(prepare_label(moves_list[count]))\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2392a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73521b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5510a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_one_hot = tf.one_hot(y, depth=19*19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b048ff",
   "metadata": {},
   "source": [
    "Dataset splitting: 90% Training, 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f594acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(x, y_one_hot.numpy(), test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c5de9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "### Simple DCNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208834da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, Flatten, Dense, Softmax\n",
    "\n",
    "# def residual_block(inputs, filters, kernel_size=3, strides=1):\n",
    "#     shortcut = inputs\n",
    "\n",
    "#     outputs = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "#     outputs = BatchNormalization()(outputs)\n",
    "#     outputs = Activation('relu')(outputs)\n",
    "\n",
    "#     outputs = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(outputs)\n",
    "#     outputs = BatchNormalization()(outputs)\n",
    "\n",
    "#     if shortcut.shape[-1] != filters:\n",
    "#         shortcut = Conv2D(filters, kernel_size=1, strides=strides, padding='same')(shortcut)\n",
    "    \n",
    "#     outputs = Add()([outputs, shortcut])\n",
    "#     outputs = Activation('relu')(outputs)\n",
    "#     return outputs\n",
    "\n",
    "# def create_model():\n",
    "#     inputs = Input(shape=(19, 19, 6))\n",
    "\n",
    "#     outputs = Conv2D(kernel_size=7, filters=32, padding='same', activation='relu')(inputs)\n",
    "#     outputs = Conv2D(kernel_size=7, filters=32, padding='same', activation='relu')(outputs)\n",
    "#     outputs = Conv2D(kernel_size=5, filters=32, padding='same', activation='relu')(outputs)\n",
    "#     outputs = Conv2D(kernel_size=5, filters=32, padding='same', activation='relu')(outputs)\n",
    "#     outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(outputs)\n",
    "#     outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(outputs)\n",
    "\n",
    "#     # Adding a residual block\n",
    "#     outputs = residual_block(outputs, filters=32)\n",
    "\n",
    "#     outputs = Conv2D(kernel_size=3, filters=1, padding='same')(outputs)\n",
    "#     outputs = Flatten()(outputs)\n",
    "#     outputs = Softmax()(outputs)\n",
    "    \n",
    "#     model = Model(inputs, outputs)\n",
    "    \n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#     model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def residual_block(inputs, filters, kernel_size=3, strides=1, dropout_rate=0.2):\n",
    "    shortcut = inputs\n",
    "\n",
    "    outputs = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "    outputs = Activation('relu')(outputs)\n",
    "    # outputs = Dropout(dropout_rate)(outputs)  # 添加 Dropout\n",
    "\n",
    "    outputs = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(outputs)\n",
    "    outputs = BatchNormalization()(outputs)\n",
    "\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, kernel_size=1, strides=strides, padding='same')(shortcut)\n",
    "    \n",
    "    outputs = Add()([outputs, shortcut])\n",
    "    outputs = Activation('relu')(outputs)\n",
    "    # outputs = Dropout(dropout_rate)(outputs)  # 添加 Dropout\n",
    "    return outputs\n",
    "\n",
    "def create_model():\n",
    "    inputs = Input(shape=(19, 19, 11))\n",
    "\n",
    "    outputs = Conv2D(kernel_size=7, filters=32, padding='same', activation='relu')(inputs)\n",
    "    outputs = Conv2D(kernel_size=7, filters=32, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=7, filters=32, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=32, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=5, filters=32, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(outputs)\n",
    "    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu')(outputs)\n",
    "\n",
    "    # Adding multiple residual blocks with dropout\n",
    "    for _ in range(3):  # 添加三个残差块\n",
    "        outputs = residual_block(outputs, filters=32, dropout_rate=0.3)  # 调整 dropout 比率\n",
    "\n",
    "    outputs = Conv2D(kernel_size=3, filters=1, padding='same')(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Softmax()(outputs)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    itial_learning_rate = 0.01\n",
    "    decay_steps = 5000  # 每隔多少步驟調整學習率\n",
    "    decay_rate = 0.9  # 學習率衰減率\n",
    "\n",
    "    \n",
    "    # # 使用 ExponentialDecay 來調整學習率\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #     initial_learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, staircase=True\n",
    "    # )\n",
    "\n",
    "    # # 創建 Adam 優化器並指定學習率\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    opt = RMSprop(learning_rate=0.001, rho=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a66e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 19, 19, 11)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 19, 19, 32)   17280       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 19, 19, 32)   50208       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 19, 19, 32)   50208       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 19, 19, 32)   25632       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 19, 19, 32)   25632       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 19, 19, 32)   9248        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 19, 19, 32)   9248        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 19, 19, 32)   9248        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 19, 19, 32)   9248        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 19, 19, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 19, 19, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 19, 19, 32)   9248        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 19, 19, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 19, 19, 32)   0           batch_normalization_1[0][0]      \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 19, 19, 32)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 19, 19, 32)   9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 19, 19, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 19, 19, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 19, 19, 32)   9248        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 19, 19, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 19, 19, 32)   0           batch_normalization_3[0][0]      \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 19, 19, 32)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 19, 19, 32)   9248        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 19, 19, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 19, 19, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 19, 19, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 19, 19, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 19, 19, 32)   0           batch_normalization_5[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 19, 19, 32)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 19, 19, 1)    289         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 361)          0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 361)          0           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 253,249\n",
      "Trainable params: 252,865\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 21:41:40.767577: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-11-28 21:41:40.767700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:40.767794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:05:00.0 name: NVIDIA GeForce RTX 4060 Ti computeCapability: 8.9\n",
      "coreClock: 2.655GHz coreCount: 34 deviceMemorySize: 15.71GiB deviceMemoryBandwidth: 268.25GiB/s\n",
      "2023-11-28 21:41:40.767808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-28 21:41:40.767820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-28 21:41:40.767825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-11-28 21:41:40.767831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-11-28 21:41:40.767835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-11-28 21:41:40.767840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-11-28 21:41:40.767845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-11-28 21:41:40.767849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-28 21:41:40.767884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:40.767940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:40.767983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 1\n",
      "2023-11-28 21:41:40.767993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-11-28 21:41:41.020880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-11-28 21:41:41.020897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      1 \n",
      "2023-11-28 21:41:41.020900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N \n",
      "2023-11-28 21:41:41.021020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:41.021100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:41.021157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-28 21:41:41.021216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14858 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:05:00.0, compute capability: 8.9)\n"
     ]
    }
   ],
   "source": [
    "# model = create_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 20:48:23.106153: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-11-28 20:48:23.106369: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2112000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 20:48:25.071293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-28 20:48:26.112094: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-28 20:48:26.113246: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 126s 123ms/step - loss: 5.2030 - accuracy: 0.0321 - val_loss: 4.4882 - val_accuracy: 0.0726\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.48819, saving model to testmodel_bestdan.h5\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 124s 124ms/step - loss: 4.1545 - accuracy: 0.1148 - val_loss: 3.9532 - val_accuracy: 0.1824\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.48819 to 3.95320, saving model to testmodel_bestdan.h5\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 3.7058 - accuracy: 0.2141 - val_loss: 3.5698 - val_accuracy: 0.2683\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.95320 to 3.56984, saving model to testmodel_bestdan.h5\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 124s 124ms/step - loss: 3.4356 - accuracy: 0.2913 - val_loss: 3.3839 - val_accuracy: 0.3102\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.56984 to 3.38391, saving model to testmodel_bestdan.h5\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 3.2139 - accuracy: 0.3196 - val_loss: 3.1470 - val_accuracy: 0.3396\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.38391 to 3.14696, saving model to testmodel_bestdan.h5\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 3.0399 - accuracy: 0.3462 - val_loss: 3.0441 - val_accuracy: 0.3487\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.14696 to 3.04408, saving model to testmodel_bestdan.h5\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 2.9061 - accuracy: 0.3572 - val_loss: 2.9191 - val_accuracy: 0.3622\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.04408 to 2.91914, saving model to testmodel_bestdan.h5\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 2.8390 - accuracy: 0.3618 - val_loss: 2.7919 - val_accuracy: 0.3726\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.91914 to 2.79189, saving model to testmodel_bestdan.h5\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 2.6985 - accuracy: 0.3864 - val_loss: 2.7951 - val_accuracy: 0.3703\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.79189\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 2.6162 - accuracy: 0.3882 - val_loss: 2.7689 - val_accuracy: 0.3708\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.79189 to 2.76889, saving model to testmodel_bestdan.h5\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 2.6470 - accuracy: 0.3832 - val_loss: 2.6817 - val_accuracy: 0.3808\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.76889 to 2.68169, saving model to testmodel_bestdan.h5\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 2.5261 - accuracy: 0.4067 - val_loss: 2.6664 - val_accuracy: 0.3864\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.68169 to 2.66644, saving model to testmodel_bestdan.h5\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 2.5690 - accuracy: 0.3950 - val_loss: 2.6268 - val_accuracy: 0.3902\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.66644 to 2.62678, saving model to testmodel_bestdan.h5\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 2.5659 - accuracy: 0.3951 - val_loss: 2.6205 - val_accuracy: 0.3917\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.62678 to 2.62051, saving model to testmodel_bestdan.h5\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 2.5002 - accuracy: 0.4059 - val_loss: 2.6033 - val_accuracy: 0.3977\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.62051 to 2.60333, saving model to testmodel_bestdan.h5\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 120s 120ms/step - loss: 2.5433 - accuracy: 0.3938 - val_loss: 2.5995 - val_accuracy: 0.3894\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.60333 to 2.59947, saving model to testmodel_bestdan.h5\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 123s 123ms/step - loss: 2.5443 - accuracy: 0.3933 - val_loss: 2.5972 - val_accuracy: 0.3974\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.59947 to 2.59721, saving model to testmodel_bestdan.h5\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 124s 124ms/step - loss: 2.4853 - accuracy: 0.4013 - val_loss: 2.6278 - val_accuracy: 0.3937\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.59721\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 122s 122ms/step - loss: 2.4815 - accuracy: 0.4072 - val_loss: 2.6205 - val_accuracy: 0.3892\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.59721\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 121s 121ms/step - loss: 2.4702 - accuracy: 0.4057 - val_loss: 2.5738 - val_accuracy: 0.4003\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.59721 to 2.57375, saving model to testmodel_bestdan.h5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def games_generator():\n",
    "    for game in games:\n",
    "        yield from process_game(game)\n",
    "\n",
    "def process_game(game):\n",
    "    moves_list = game.split(',')\n",
    "    for count, move in enumerate(moves_list):\n",
    "        input_data = prepare_input(moves_list[:count])\n",
    "        label = prepare_label(moves_list[count])\n",
    "        yield input_data, label\n",
    "\n",
    "\n",
    "games_dataset = tf.data.Dataset.from_generator(\n",
    "    games_generator,\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=([19, 19, 11], [])\n",
    ")\n",
    "\n",
    "\n",
    "# 將數據集分成訓練和驗證集\n",
    "val_dataset = games_dataset.take(10000) \n",
    "train_dataset = games_dataset.skip(10000)\n",
    "\n",
    "# 對數據集進行批處理和預處理\n",
    "train_dataset = train_dataset.batch(512).map(lambda x, y: (x, tf.one_hot(y, depth=19*19)))\n",
    "val_dataset = val_dataset.batch(512).map(lambda x, y: (x, tf.one_hot(y, depth=19*19)))\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpoint_path = \"testmodel_bestdan.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='val_loss',  # 監控準確度\n",
    "                             save_best_only=True,     # 只保存最佳模型\n",
    "                             mode='min',              # 目標是最大化監控指標\n",
    "                             verbose=1)\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset,\n",
    "    steps_per_epoch=1000,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4d7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 10:26:03.647589: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 10559286100 exceeds 10% of free system memory.\n",
      "2023-11-21 10:26:06.949069: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-11-21 10:26:06.949291: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2112000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 10:26:07.171023: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-11-21 10:26:09.054404: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-11-21 10:26:09.055742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5713/5713 [==============================] - 68s 12ms/step - loss: 4.2732 - accuracy: 0.1372 - val_loss: 3.3285 - val_accuracy: 0.2958\n",
      "Epoch 2/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 3.2489 - accuracy: 0.3027 - val_loss: 3.1152 - val_accuracy: 0.3209\n",
      "Epoch 3/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 3.0471 - accuracy: 0.3271 - val_loss: 3.0073 - val_accuracy: 0.3342\n",
      "Epoch 4/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.9361 - accuracy: 0.3400 - val_loss: 2.8964 - val_accuracy: 0.3469\n",
      "Epoch 5/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.8607 - accuracy: 0.3503 - val_loss: 2.8977 - val_accuracy: 0.3480\n",
      "Epoch 6/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.8080 - accuracy: 0.3561 - val_loss: 2.8016 - val_accuracy: 0.3589\n",
      "Epoch 7/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.7691 - accuracy: 0.3602 - val_loss: 2.7740 - val_accuracy: 0.3600\n",
      "Epoch 8/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.7375 - accuracy: 0.3637 - val_loss: 2.7626 - val_accuracy: 0.3606\n",
      "Epoch 9/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.7124 - accuracy: 0.3671 - val_loss: 2.7402 - val_accuracy: 0.3659\n",
      "Epoch 10/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.6922 - accuracy: 0.3686 - val_loss: 2.7272 - val_accuracy: 0.3651\n",
      "Epoch 11/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.6766 - accuracy: 0.3709 - val_loss: 2.7161 - val_accuracy: 0.3677\n",
      "Epoch 12/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.6622 - accuracy: 0.3728 - val_loss: 2.7031 - val_accuracy: 0.3693\n",
      "Epoch 13/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.6486 - accuracy: 0.3751 - val_loss: 2.7000 - val_accuracy: 0.3694\n",
      "Epoch 14/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.6387 - accuracy: 0.3755 - val_loss: 2.6820 - val_accuracy: 0.3705\n",
      "Epoch 15/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.6313 - accuracy: 0.3772 - val_loss: 2.6880 - val_accuracy: 0.3713\n",
      "Epoch 16/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.6222 - accuracy: 0.3787 - val_loss: 2.6698 - val_accuracy: 0.3738\n",
      "Epoch 17/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.6136 - accuracy: 0.3794 - val_loss: 2.6778 - val_accuracy: 0.3723\n",
      "Epoch 18/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.6062 - accuracy: 0.3806 - val_loss: 2.6661 - val_accuracy: 0.3739\n",
      "Epoch 19/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.6041 - accuracy: 0.3812 - val_loss: 2.6658 - val_accuracy: 0.3745\n",
      "Epoch 20/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.5954 - accuracy: 0.3820 - val_loss: 2.6697 - val_accuracy: 0.3765\n",
      "Epoch 21/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.5916 - accuracy: 0.3829 - val_loss: 2.6523 - val_accuracy: 0.3758\n",
      "Epoch 22/25\n",
      "5713/5713 [==============================] - 66s 11ms/step - loss: 2.5873 - accuracy: 0.3835 - val_loss: 2.6657 - val_accuracy: 0.3758\n",
      "Epoch 23/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.5841 - accuracy: 0.3839 - val_loss: 2.6416 - val_accuracy: 0.3789\n",
      "Epoch 24/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.5766 - accuracy: 0.3850 - val_loss: 2.6318 - val_accuracy: 0.3792\n",
      "Epoch 25/25\n",
      "5713/5713 [==============================] - 65s 11ms/step - loss: 2.5737 - accuracy: 0.3857 - val_loss: 2.6373 - val_accuracy: 0.3769\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(\n",
    "#     x = x_train, \n",
    "#     y = y_train,\n",
    "#     batch_size = 256,\n",
    "#     epochs = 25,\n",
    "#     validation_data=(x_val, y_val),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aaddf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_dan_tutorial5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484be28d",
   "metadata": {},
   "source": [
    "## ALL DONE!\n",
    "\n",
    "For using the model and creating a submission file, follow the notebook **Create Public Upload CSV.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7fafaa",
   "metadata": {},
   "source": [
    "# End of Tutorial\n",
    "\n",
    "You are free to use more modern NN architectures, a better pre-processing, feature extraction methods to achieve much better accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b4672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
